{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP2200/COMP6200 Practical Exercise (Week 12): Recommender Systems\n",
    "\n",
    "## School of Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEU Survey\n",
    "\n",
    "You have been invited to participate in a Learner Experience of Unit (LEU) survey for COMP2200/COMP6200-Data Science. You will have already received an individual email invitation and link to the survey. You can also access the survey on the 'Student Feedback Surveys' block of your iLearn homepage (not the unit homepage). Take some time to complete it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prac Details\n",
    "\n",
    "This prac is almost definitely more than 2 hours in length. This is the first time we have done taught recommendation engines, so we don't know how much of the prac students will complete within 2 hours.\n",
    "\n",
    "Feel free to choose which sections are the most interesting to you. Let us know which parts you found useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Choose your environment: either run locally with `uv` or use [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "1. **Local with `uv`**:\n",
    "   - Install `uv`:\n",
    "     - **MacOS/Linux**: `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "     - **Windows**: `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n",
    "   - Create a folder for this practical (for example, `week12-prac`) and open a terminal in it.\n",
    "   - Set up your environment:\n",
    "     ```\n",
    "     uv init\n",
    "     uv add implicit pandas numpy matplotlib scipy scikit-learn\n",
    "     ```\n",
    "   - Launch Jupyter with `uv run --with jupyter jupyter lab` (or `... notebook`).\n",
    "\n",
    "2. **Google Colab**:\n",
    "   - Open Colab and create a new notebook.\n",
    "   - Install packages with `!pip install implicit pandas numpy matplotlib scipy scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A --- Joke Recommender with Jester Dataset\n",
    "\n",
    "In this part, you'll build a joke recommendation system using the Jester dataset. Jester contains ratings of jokes from real users on a continuous scale from -10 (worst joke ever) to +10 (best joke ever). This is a classic example of **explicit feedback** --- users directly tell us their opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1. Import required libraries\n",
    "\n",
    "Import the necessary Python modules for this practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Load the Jester dataset\n",
    "\n",
    "Load the Jester dataset from the local file and examine its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Jester dataset...\")\n",
    "jester_raw = pd.read_csv('jester-data-1.csv', header=None, usecols=range(1, 101))\n",
    "print(\"Dataset loaded!\")\n",
    "\n",
    "# Get dimensions\n",
    "n_users, n_jokes = jester_raw.shape\n",
    "print(f\"Number of users: {n_users:,}\")\n",
    "print(f\"Number of jokes: {n_jokes}\")\n",
    "print(f\"\\nSample of raw data (99 means 'not rated'):\")\n",
    "print(jester_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Explore the dataset statistics\n",
    "\n",
    "Convert from wide to long format using pandas `melt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from wide to long format\n",
    "# Columns are jokes (0-99), rows are users, 99 means \"not rated\"\n",
    "\n",
    "# Add user ID column from the index\n",
    "jester_raw['user'] = jester_raw.index\n",
    "\n",
    "ratings_df = jester_raw.melt(\n",
    "    id_vars='user',\n",
    "    var_name='item',\n",
    "    value_name='rating'\n",
    ")\n",
    "\n",
    "# Filter out missing ratings (99 means not rated)\n",
    "ratings_df = ratings_df[ratings_df['rating'] != 99]\n",
    "\n",
    "# Convert item to 0-indexed integers (columns are 1-100, need 0-99 for indexing)\n",
    "ratings_df['item'] = ratings_df['item'].astype(int) - 1\n",
    "\n",
    "# Print statistics\n",
    "n_ratings = len(ratings_df)\n",
    "\n",
    "print(f\"Number of ratings: {n_ratings:,}\")\n",
    "print(f\"Rating scale: {ratings_df['rating'].min():.1f} to {ratings_df['rating'].max():.1f}\")\n",
    "print(f\"Global mean rating: {ratings_df['rating'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the matrix sparsity (what percentage of cells are empty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_possible = n_users * n_jokes\n",
    "sparsity = 1 - (n_ratings / total_possible)\n",
    "print(f\"Matrix sparsity: {sparsity:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The Jester dataset has about 27% sparsity. Typical movie ratings datasets (like MovieLens) have 93--95% sparsity. Why is Jester so much less sparse? What does this tell you about how the Jester data was collected compared to movie ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4. Train a baseline model\n",
    "\n",
    "Let's start with a baseline: a simple model that always predicts the mean rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test sets\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=1000, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df):,}\")\n",
    "print(f\"Test set: {len(test_df):,}\")\n",
    "\n",
    "# Train baseline model (always predicts mean rating)\n",
    "baseline = DummyRegressor(strategy='mean')\n",
    "baseline.fit(train_df[['user', 'item']], train_df['rating'])\n",
    "y_pred = baseline.predict(test_df[['user', 'item']])\n",
    "\n",
    "# Evaluate\n",
    "rmse_baseline = root_mean_squared_error(test_df['rating'], y_pred)\n",
    "mae_baseline = mean_absolute_error(test_df['rating'], y_pred)\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"  RMSE: {rmse_baseline:.4f}\")\n",
    "print(f\"  MAE:  {mae_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A5. Train an SVD model\n",
    "\n",
    "SVD (Singular Value Decomposition) is a matrix factorization technique that finds latent factors for users and jokes. The idea is that each user and each joke can be represented as a vector in a lower-dimensional \"latent space\". Users and jokes that are similar will be close together in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create the user-item matrix\n",
    "\n",
    "First, create a user-item matrix from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-item matrix from training data\n",
    "# Rows = users, columns = jokes, cells = ratings\n",
    "user_item_matrix = train_df.pivot_table(\n",
    "    index='user',\n",
    "    columns='item',\n",
    "    values='rating',\n",
    "    fill_value=0  # Fill missing ratings with 0\n",
    ")\n",
    "\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pivot_table` operation converts our long-format dataframe (one row per rating) into a wide-format matrix where each row represents a user and each column represents a joke. Missing ratings are filled with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply SVD decomposition\n",
    "\n",
    "Now apply SVD to decompose this matrix into user and item factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD using scikit-learn's TruncatedSVD\n",
    "# n_factors controls the dimensionality of the latent space\n",
    "# More factors = more expressive but risk of overfitting\n",
    "n_factors = 20\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "user_factors = svd_model.fit_transform(user_item_matrix)\n",
    "item_factors = svd_model.components_.T\n",
    "\n",
    "print(f\"User factors shape: {user_factors.shape}\")\n",
    "print(f\"Item factors shape: {item_factors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD decomposes the user-item matrix into two smaller matrices: `user_factors` (users \u00d7 latent factors) and `item_factors` (jokes \u00d7 latent factors). Each user is now represented by 20 numbers, and each joke is represented by 20 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Reconstruct the ratings matrix\n",
    "\n",
    "Reconstruct the full ratings matrix and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the full ratings matrix by multiplying user and item factors\n",
    "# Matrix multiplication: (users x factors) @ (factors x items) = (users x items)\n",
    "# This gives us predicted ratings for all user-item pairs\n",
    "predicted_ratings = user_factors @ item_factors.T\n",
    "\n",
    "print(f\"Predicted ratings matrix shape: {predicted_ratings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructed matrix contains predicted ratings for every user-joke pair, even those we never observed in the training data. This is the power of collaborative filtering --- it can predict ratings for unseen user-joke combinations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate on test set\n",
    "\n",
    "Now evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for our test set\n",
    "# For each test rating, look up the predicted value for that (user, item) pair\n",
    "test_users = test_df['user'].values\n",
    "test_items = test_df['item'].values\n",
    "svd_predictions = predicted_ratings[test_users, test_items]\n",
    "\n",
    "# Calculate error metrics\n",
    "rmse_svd = root_mean_squared_error(test_df['rating'], svd_predictions)\n",
    "mae_svd = mean_absolute_error(test_df['rating'], svd_predictions)\n",
    "\n",
    "print(f\"\\nSVD Results:\")\n",
    "print(f\"  RMSE: {rmse_svd:.4f}\")\n",
    "print(f\"  MAE:  {mae_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the improvement over the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement = (1 - rmse_svd/rmse_baseline) * 100\n",
    "print(f\"\\nImprovement over baseline: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How much better is SVD than the baseline (predicting mean rating)? What does this tell you about patterns in humor preferences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A6. Train a KNN model\n",
    "\n",
    "KNN (K-Nearest Neighbors) implements user-based collaborative filtering. The idea is simple: to predict what rating you'll give a joke, we find users similar to you and use their ratings to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build the KNN model\n",
    "\n",
    "First, build the KNN model to find similar users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KNN model on user vectors\n",
    "# Each user is represented by their ratings across all jokes\n",
    "k = 30  # How many similar users to consider\n",
    "knn_model = NearestNeighbors(n_neighbors=k+1, metric='cosine')\n",
    "knn_model.fit(user_item_matrix.values)\n",
    "\n",
    "# Calculate global mean from training data for fallback\n",
    "# We'll use this if we can't find similar users who rated an item\n",
    "global_mean = train_df['rating'].mean()\n",
    "print(f\"Global mean rating: {global_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity measures how similar two users are based on their rating patterns. The model will find the k=30 most similar users for any given user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make predictions\n",
    "\n",
    "Now make predictions for each test rating by averaging similar users' ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for each test rating\n",
    "knn_predictions = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = int(row['user'])\n",
    "    item_id = int(row['item'])\n",
    "\n",
    "    # Find this user's k nearest neighbors\n",
    "    user_vector = user_item_matrix.iloc[user_id].values.reshape(1, -1)\n",
    "    distances, neighbor_indices = knn_model.kneighbors(user_vector)\n",
    "\n",
    "    # Get ratings from neighbors for this specific joke\n",
    "    neighbor_ratings = []\n",
    "    for neighbor_idx in neighbor_indices[0][1:]:  # Skip user itself (first result)\n",
    "        rating = user_item_matrix.iloc[neighbor_idx, item_id]\n",
    "        if rating != 0:  # Only use actual ratings (0 means missing)\n",
    "            neighbor_ratings.append(rating)\n",
    "\n",
    "    # Predict as average of neighbor ratings, or global mean if no neighbors rated it\n",
    "    if len(neighbor_ratings) > 0:\n",
    "        knn_predictions.append(np.mean(neighbor_ratings))\n",
    "    else:\n",
    "        knn_predictions.append(global_mean)\n",
    "\n",
    "knn_predictions = np.array(knn_predictions)\n",
    "print(f\"Made {len(knn_predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction algorithm works like this: For each test rating, we find similar users who also rated that joke, then average their ratings. If no similar users rated it, we fall back to the global mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate performance\n",
    "\n",
    "Evaluate the KNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error metrics\n",
    "rmse_knn = root_mean_squared_error(test_df['rating'], knn_predictions)\n",
    "mae_knn = mean_absolute_error(test_df['rating'], knn_predictions)\n",
    "\n",
    "print(f\"KNN Results:\")\n",
    "print(f\"  RMSE: {rmse_knn:.4f}\")\n",
    "print(f\"  MAE:  {mae_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A7. Try content-based filtering with TF-IDF\n",
    "\n",
    "So far we've used collaborative filtering (learning from user ratings). Now let's try content-based filtering (using the text of jokes).\n",
    "\n",
    "First, load the joke text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load joke text\n",
    "jokes_df = pd.read_csv('jester-jokes.csv')\n",
    "print(f\"Loaded {len(jokes_df)} jokes\")\n",
    "print(f\"\\nSample joke:\")\n",
    "print(f\"Joke {jokes_df.iloc[0]['joke_id']}: {jokes_df.iloc[0]['joke_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build TF-IDF vectors and compute joke similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Build TF-IDF vectors for jokes\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(jokes_df['joke_text'])\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Compute joke-to-joke similarity\n",
    "joke_similarity = cosine_similarity(tfidf_matrix)\n",
    "print(f\"Joke similarity matrix shape: {joke_similarity.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using content-based filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based predictions:\n",
    "# For each test rating, find jokes the user liked and recommend similar jokes\n",
    "print(\"Making content-based predictions...\")\n",
    "cb_predictions = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = row['user']\n",
    "    item_id = int(row['item'])\n",
    "\n",
    "    # Get user's training ratings\n",
    "    user_train = train_df[train_df['user'] == user_id]\n",
    "\n",
    "    if len(user_train) == 0:\n",
    "        # Cold start: use global mean\n",
    "        cb_predictions.append(train_df['rating'].mean())\n",
    "        continue\n",
    "\n",
    "    # Weight user's ratings by joke similarity\n",
    "    similarities = joke_similarity[item_id]\n",
    "    weighted_sum = 0\n",
    "    weight_total = 0\n",
    "\n",
    "    for _, train_row in user_train.iterrows():\n",
    "        train_item = int(train_row['item'])\n",
    "        train_rating = train_row['rating']\n",
    "        similarity = similarities[train_item]\n",
    "\n",
    "        if similarity > 0:\n",
    "            weighted_sum += similarity * train_rating\n",
    "            weight_total += similarity\n",
    "\n",
    "    if weight_total > 0:\n",
    "        cb_predictions.append(weighted_sum / weight_total)\n",
    "    else:\n",
    "        cb_predictions.append(train_df['rating'].mean())\n",
    "\n",
    "cb_predictions = np.array(cb_predictions)\n",
    "\n",
    "rmse_cb = root_mean_squared_error(test_df['rating'], cb_predictions)\n",
    "mae_cb = mean_absolute_error(test_df['rating'], cb_predictions)\n",
    "\n",
    "print(f\"\\nContent-Based (TF-IDF) Results:\")\n",
    "print(f\"  RMSE: {rmse_cb:.4f}\")\n",
    "print(f\"  MAE:  {mae_cb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine what TF-IDF considers \"similar\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show similar jokes by TF-IDF\n",
    "example_id = 0\n",
    "print(f\"\\nJoke {example_id}: {jokes_df.iloc[example_id]['joke_text']}\")\n",
    "\n",
    "similarities = joke_similarity[example_id]\n",
    "similar_indices = np.argsort(similarities)[::-1][1:4]  # Top 3 (excluding self)\n",
    "\n",
    "print(f\"\\nTop 3 most similar jokes by TF-IDF:\")\n",
    "for rank, idx in enumerate(similar_indices, 1):\n",
    "    print(f\"\\n{rank}. Joke {idx} (similarity: {similarities[idx]:.3f})\")\n",
    "    print(f\"   {jokes_df.iloc[idx]['joke_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Are the \"similar\" jokes actually funny in the same way? What does TF-IDF capture about jokes, and what does it miss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore what words appear in funny vs unfunny jokes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge jokes with their mean ratings\n",
    "joke_stats = ratings_df.groupby('item')['rating'].agg(['mean', 'count'])\n",
    "joke_stats = joke_stats.sort_values('mean', ascending=False)\n",
    "jokes_with_ratings = jokes_df.merge(\n",
    "    joke_stats, left_on='joke_id', right_index=True\n",
    ")\n",
    "\n",
    "# Sort by rating to get actual funniest/worst\n",
    "jokes_with_ratings = jokes_with_ratings.sort_values('mean', ascending=False)\n",
    "\n",
    "# Show funniest and worst jokes\n",
    "print(\"Top 5 funniest jokes:\")\n",
    "for i, row in jokes_with_ratings.head(5).iterrows():\n",
    "    print(f\"\\nJoke {row['joke_id']} (rating: {row['mean']:+.2f}):\")\n",
    "    print(f\"  {row['joke_text']}\")\n",
    "\n",
    "print(\"\\n\\nTop 5 worst jokes:\")\n",
    "for i, row in jokes_with_ratings.tail(5).iterrows():\n",
    "    print(f\"\\nJoke {row['joke_id']} (rating: {row['mean']:+.2f}):\")\n",
    "    print(f\"  {row['joke_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TF-IDF with Ridge Regression to find words associated with humor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build word-level TF-IDF features\n",
    "\n",
    "First, import Ridge Regression and build TF-IDF features for individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF + Ridge Regression to find words associated with humor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Build TF-IDF features for all jokes (individual words only)\n",
    "vectorizer_words = TfidfVectorizer(\n",
    "    max_features=100,      # Keep only top 100 words\n",
    "    stop_words='english',  # Remove common words like \"the\", \"a\", \"is\"\n",
    "    ngram_range=(1, 1),    # Single words only (unigrams)\n",
    "    min_df=2               # Word must appear in at least 2 jokes\n",
    ")\n",
    "\n",
    "X_words = vectorizer_words.fit_transform(jokes_with_ratings['joke_text'])\n",
    "y_ratings = jokes_with_ratings['mean'].values\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_words.shape}\")\n",
    "print(f\"Target ratings shape: {y_ratings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numbers. Each joke becomes a vector of 100 numbers representing how important each word is in that joke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Ridge Regression on words\n",
    "\n",
    "Now train a Ridge Regression model to predict joke ratings from words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge regression to predict ratings from words\n",
    "# RidgeCV automatically selects the best regularization strength\n",
    "ridge = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0])\n",
    "ridge.fit(X_words, y_ratings)\n",
    "\n",
    "print(f\"\\nWord-based rating prediction R^2: {ridge.score(X_words, y_ratings):.4f}\")\n",
    "print(f\"Best alpha: {ridge.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R\u00b2 score tells us how well words predict joke ratings. A score near 1.0 means words are very predictive; near 0 means they're not helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Analyze word coefficients\n",
    "\n",
    "Examine which words are associated with funny vs unfunny jokes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients (positive = associated with funny, negative = unfunny)\n",
    "coefs = ridge.coef_\n",
    "feature_names = vectorizer_words.get_feature_names_out()\n",
    "sorted_indices = np.argsort(coefs)\n",
    "\n",
    "print(\"\\nTop 10 words associated with FUNNY jokes:\")\n",
    "for idx in sorted_indices[-10:][::-1]:\n",
    "    print(f\"  {feature_names[idx]:20s}: {coefs[idx]:+.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 words associated with UNFUNNY jokes:\")\n",
    "for idx in sorted_indices[:10]:\n",
    "    print(f\"  {feature_names[idx]:20s}: {coefs[idx]:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive coefficients mean the word appears more in highly-rated jokes; negative coefficients mean it appears more in poorly-rated jokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build bigram features\n",
    "\n",
    "Now try bigrams (two-word phrases) instead of individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF features for bigrams (two-word phrases)\n",
    "vectorizer_bigrams = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 2),  # Only two-word phrases\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X_bigrams = vectorizer_bigrams.fit_transform(jokes_with_ratings['joke_text'])\n",
    "print(f\"Bigram TF-IDF matrix shape: {X_bigrams.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams can capture phrases like \"knock knock\" or \"chicken cross\" that might be more meaningful than individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train and evaluate bigram model\n",
    "\n",
    "Train and evaluate the bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge regression with bigrams\n",
    "ridge_bigrams = RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0, 100.0])\n",
    "ridge_bigrams.fit(X_bigrams, y_ratings)\n",
    "\n",
    "print(f\"\\nBigram-based rating prediction R^2: {ridge_bigrams.score(X_bigrams, y_ratings):.4f}\")\n",
    "\n",
    "# Show top bigrams\n",
    "coefs_bigrams = ridge_bigrams.coef_\n",
    "features_bigrams = vectorizer_bigrams.get_feature_names_out()\n",
    "sorted_indices_bigrams = np.argsort(coefs_bigrams)\n",
    "\n",
    "print(\"\\nTop 10 bigrams associated with FUNNY jokes:\")\n",
    "for idx in sorted_indices_bigrams[-10:][::-1]:\n",
    "    print(f\"  {features_bigrams[idx]:30s}: {coefs_bigrams[idx]:+.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 bigrams associated with UNFUNNY jokes:\")\n",
    "for idx in sorted_indices_bigrams[:10]:\n",
    "    print(f\"  {features_bigrams[idx]:30s}: {coefs_bigrams[idx]:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Look at the funniest vs worst jokes. Can you identify patterns? Are funny jokes longer stories while unfunny ones are short Q&A puns? Can word counts alone predict humor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A8. Compare models with a bar chart\n",
    "\n",
    "Visualize the performance of all four models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Baseline', 'Content-Based', 'KNN', 'SVD']\n",
    "rmse_values = [rmse_baseline, rmse_cb, rmse_knn, rmse_svd]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(models, rmse_values, color=['gray', 'orange', 'lightblue', 'lightgreen'],\n",
    "        edgecolor='black', alpha=0.7)\n",
    "plt.ylabel('RMSE (Lower is Better)')\n",
    "plt.title('Model Comparison on Jester Dataset')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(rmse_values):\n",
    "    plt.text(i, v + 0.05, f'{v:.4f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model performs best? Which model is worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A9. Analyze joke ratings\n",
    "\n",
    "The SVD model is already trained on the full dataset. Let's analyze joke ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean ratings per joke\n",
    "joke_stats = ratings_df.groupby('item')['rating'].agg(['mean', 'count'])\n",
    "joke_stats = joke_stats.sort_values('mean', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 funniest jokes (highest average rating):\")\n",
    "for idx, (joke_id, row) in enumerate(joke_stats.head(5).iterrows(), 1):\n",
    "    print(f\"  {idx}. Joke {joke_id}: avg rating = {row['mean']:+.2f} ({int(row['count'])} ratings)\")\n",
    "\n",
    "print(f\"\\nTop 5 worst jokes (lowest average rating):\")\n",
    "for idx, (joke_id, row) in enumerate(joke_stats.tail(5).iterrows(), 1):\n",
    "    print(f\"  {idx}. Joke {joke_id}: avg rating = {row['mean']:+.2f} ({int(row['count'])} ratings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A10. Generate personalized joke recommendations\n",
    "\n",
    "Use the SVD model to recommend jokes for a specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for user 0\n",
    "user_id = 0\n",
    "\n",
    "# Get user's predictions from the reconstructed ratings matrix\n",
    "user_predictions = predicted_ratings[user_id, :]\n",
    "\n",
    "# Find items the user hasn't rated yet\n",
    "user_rated_items = ratings_df[ratings_df['user'] == user_id]['item'].values\n",
    "unrated_items = [i for i in range(n_jokes) if i not in user_rated_items]\n",
    "\n",
    "# Get predictions for unrated items and sort\n",
    "unrated_predictions = [(item, user_predictions[item]) for item in unrated_items]\n",
    "unrated_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show top 10\n",
    "print(f\"\\nTop 10 joke recommendations for User {user_id}:\")\n",
    "for i, (joke_id, pred_rating) in enumerate(unrated_predictions[:10], 1):\n",
    "    print(f\"  {i}. Joke {joke_id}: predicted rating = {pred_rating:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: If you were building a real joke app, how would you present these recommendations to users? Would you just show the top-rated jokes, or try to balance quality with diversity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B --- Implicit Feedback Comparison\n",
    "\n",
    "In Part A, we used **explicit feedback** (users rated jokes). Now we'll explore **implicit feedback**, where we only observe user behavior (clicks, views) without explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B1. Import the implicit library\n",
    "\n",
    "Import the `implicit` library (you already installed it in Preparation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit.als\n",
    "import implicit.evaluation\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Load MovieLens for implicit feedback\n",
    "\n",
    "We'll use MovieLens 100K, but convert it to implicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MovieLens 100K dataset\n",
    "print(\"Loading MovieLens 100K dataset...\")\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n",
    "ml_data = pd.read_csv(url, sep='\\t', names=['user', 'item', 'rating', 'timestamp'])\n",
    "\n",
    "# Convert to 0-indexed\n",
    "ml_data['user'] = ml_data['user'] - 1\n",
    "ml_data['item'] = ml_data['item'] - 1\n",
    "\n",
    "n_users_ml = ml_data['user'].nunique()\n",
    "n_items_ml = ml_data['item'].nunique()\n",
    "\n",
    "print(f\"MovieLens 100K:\")\n",
    "print(f\"  Users: {n_users_ml}\")\n",
    "print(f\"  Movies: {n_items_ml}\")\n",
    "print(f\"  Ratings: {len(ml_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3. Convert to implicit interactions\n",
    "\n",
    "Convert ratings to binary interactions: any rating \u2265 4 means \"watched and liked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to implicit interactions (rating >= 4 means positive interaction)\n",
    "implicit_data = ml_data[ml_data['rating'] >= 4.0].copy()\n",
    "\n",
    "print(f\"Positive interactions (rating >= 4): {len(implicit_data):,}\")\n",
    "print(f\"This is {len(implicit_data)/len(ml_data)*100:.1f}% of all ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Create sparse user-item matrix\n",
    "\n",
    "Build the sparse matrix that the `implicit` library expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse user-item matrix\n",
    "user_item_matrix = csr_matrix(\n",
    "    (np.ones(len(implicit_data)),\n",
    "     (implicit_data['user'].values, implicit_data['item'].values)),\n",
    "    shape=(n_users_ml, n_items_ml)\n",
    ")\n",
    "\n",
    "print(f\"Matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Sparsity: {100 * (1 - user_item_matrix.nnz / (n_users_ml * n_items_ml)):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B5. Train ALS model\n",
    "\n",
    "Train an Alternating Least Squares model on the implicit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_als = implicit.als.AlternatingLeastSquares(\n",
    "    factors=50,\n",
    "    regularization=0.01,\n",
    "    iterations=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Note: implicit library expects item-user matrix (transposed!)\n",
    "model_als.fit(user_item_matrix.T)\n",
    "print(\"ALS training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B6. Evaluate with ranking metrics\n",
    "\n",
    "For implicit data, we can't use RMSE (there are no rating values!). Instead, we use ranking metrics like Precision@K, which measures: \"Of the top K recommendations, how many were actually relevant?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Split data into train and test\n",
    "\n",
    "First, split the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split implicit data into train/test for evaluation\n",
    "# For each user, hold out 20% of their interactions for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(implicit_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train interactions: {len(train_data):,}\")\n",
    "print(f\"Test interactions: {len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create sparse matrices\n",
    "\n",
    "Now create separate sparse matrices for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train matrix (used for model training)\n",
    "train_user_item = csr_matrix(\n",
    "    (np.ones(len(train_data)),\n",
    "     (train_data['user'].values, train_data['item'].values)),\n",
    "    shape=(n_users_ml, n_items_ml)\n",
    ")\n",
    "\n",
    "# Create test matrix (held out for evaluation)\n",
    "test_user_item = csr_matrix(\n",
    "    (np.ones(len(test_data)),\n",
    "     (test_data['user'].values, test_data['item'].values)),\n",
    "    shape=(n_users_ml, n_items_ml)\n",
    ")\n",
    "\n",
    "print(f\"Train matrix shape: {train_user_item.shape}\")\n",
    "print(f\"Test matrix shape: {test_user_item.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix format stores only the non-zero entries (user-item interactions), which is memory-efficient for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Re-train ALS model\n",
    "\n",
    "Re-train the ALS model on training data only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train model on training data only\n",
    "model_als = implicit.als.AlternatingLeastSquares(\n",
    "    factors=50,           # Latent factors (like SVD)\n",
    "    regularization=0.01,  # Prevent overfitting\n",
    "    iterations=20,        # How many training iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Note: implicit library expects item-user matrix (transposed!)\n",
    "model_als.fit(train_user_item.T)\n",
    "print(\"ALS training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate with Precision@K\n",
    "\n",
    "Evaluate using Precision@10 metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set using Precision@K\n",
    "# Precision@10 = \"Of the top 10 recommendations, how many were actually relevant?\"\n",
    "# Note: precision_at_k may have compatibility issues with some scipy versions\n",
    "try:\n",
    "    precision = implicit.evaluation.precision_at_k(\n",
    "        model_als, train_user_item, test_user_item, K=10,\n",
    "        show_progress=False, num_threads=1\n",
    "    )\n",
    "    print(f\"\\nPrecision@10: {precision:.4f}\")\n",
    "    print(f\"Interpretation: {precision*100:.1f}% of top-10 recommendations are relevant\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Precision evaluation skipped due to library compatibility issue\")\n",
    "    print(f\"Error: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B7. Get recommendations from implicit model\n",
    "\n",
    "Generate recommendations for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = 0  # First user\n",
    "\n",
    "# Get recommendations (use training data to filter out items already seen)\n",
    "item_ids, scores = model_als.recommend(user_idx, train_user_item[user_idx], N=10)\n",
    "\n",
    "print(f\"\\nTop 10 recommendations for User {user_idx}:\")\n",
    "for i, (item_idx, score) in enumerate(zip(item_ids, scores), 1):\n",
    "    print(f\"  {i}. Movie {item_idx}: confidence score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How do these \"confidence scores\" differ from the predicted ratings in Part A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B8. Compare explicit vs implicit approaches\n",
    "\n",
    "Create a comparison table in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Aspect': ['Data Type', 'Signal Strength', 'Data Volume',\n",
    "               'Goal', 'Evaluation', 'Example'],\n",
    "    'Explicit (Jester)': ['Star ratings (-10 to +10)', 'Strong', 'Moderate',\n",
    "                          'Predict rating value', 'RMSE, MAE', 'Netflix ratings'],\n",
    "    'Implicit (MovieLens)': ['Binary (watched/not)', 'Weak', 'Abundant',\n",
    "                             'Rank by preference', 'Precision@K', 'YouTube views']\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy Extensions (Optional)\n",
    "\n",
    "If you have time, try this personalized recommendation exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate some jokes and get recommendations\n",
    "\n",
    "Let's make this personal! Rate a few jokes from the database and get recommendations for jokes you might like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Rate some jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few random jokes for you to rate\n",
    "import random\n",
    "\n",
    "print(\"Let's find jokes you'll like! Rate these jokes on a scale from -10 to +10:\")\n",
    "print(\"(-10 = terrible, 0 = neutral, +10 = hilarious)\\n\")\n",
    "\n",
    "# Select 5 random jokes from the dataset\n",
    "random_joke_ids = random.sample(range(n_jokes), 5)\n",
    "your_ratings = {}\n",
    "\n",
    "for i, joke_id in enumerate(random_joke_ids, 1):\n",
    "    joke_text = jokes_df[jokes_df['joke_id'] == joke_id]['joke_text'].values[0]\n",
    "    print(f\"--- Joke {i}/5 (ID: {joke_id}) ---\")\n",
    "    print(joke_text)\n",
    "    print()\n",
    "\n",
    "    # Get rating from user\n",
    "    while True:\n",
    "        try:\n",
    "            rating = float(input(f\"Your rating (-10 to +10): \"))\n",
    "            if -10 <= rating <= 10:\n",
    "                your_ratings[joke_id] = rating\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a number between -10 and +10\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nThanks for rating! Now let's find jokes you'll love...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate personalized recommendations\n",
    "\n",
    "Now use the SVD model to predict ratings for all unrated jokes and recommend the best ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new user vector based on your ratings\n",
    "# Start with a zero vector\n",
    "your_user_vector = np.zeros(n_jokes)\n",
    "for joke_id, rating in your_ratings.items():\n",
    "    your_user_vector[joke_id] = rating\n",
    "\n",
    "# Project your ratings into the latent space using SVD\n",
    "your_factors = svd_model.transform(your_user_vector.reshape(1, -1))\n",
    "\n",
    "# Predict ratings for all jokes\n",
    "your_predictions = your_factors @ item_factors.T\n",
    "your_predictions = your_predictions[0]  # Flatten to 1D array\n",
    "\n",
    "# Find unrated jokes and sort by predicted rating\n",
    "unrated_jokes = [j for j in range(n_jokes) if j not in your_ratings]\n",
    "joke_predictions = [(j, your_predictions[j]) for j in unrated_jokes]\n",
    "joke_predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show top 5 recommendations\n",
    "print(\"Based on your ratings, here are 5 jokes we think you'll love:\\n\")\n",
    "for i, (joke_id, pred_rating) in enumerate(joke_predictions[:5], 1):\n",
    "    joke_text = jokes_df[jokes_df['joke_id'] == joke_id]['joke_text'].values[0]\n",
    "    print(f\"--- Recommendation {i} (Joke {joke_id}, predicted rating: {pred_rating:+.2f}) ---\")\n",
    "    print(joke_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do the recommended jokes match your sense of humor? If you rated them, how close were the predictions to your actual ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Questions (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Cold Start Problem**: How would you handle a brand new user who has never rated any jokes? What about a new joke that no one has rated yet?\n",
    "\n",
    "2. **Explicit vs Implicit**: YouTube uses implicit feedback (watch time, clicks) instead of asking users to rate videos. What are the advantages and disadvantages of this approach?\n",
    "\n",
    "3. **Ethics**: The lecture discussed how YouTube optimizes for engagement (watch time) while Netflix optimizes for satisfaction. How might this difference affect what content gets recommended? Can you think of potential harms from optimizing purely for engagement?\n",
    "\n",
    "4. **Filter Bubbles**: If a recommender system only shows you content similar to what you've liked before, what problems might this cause? How would you modify a recommender system to promote diversity while still being useful?\n",
    "\n",
    "5. **Evaluation Metrics**: We used RMSE for explicit ratings and Precision@K for implicit feedback. Why are these different? What does each metric actually measure?\n",
    "\n",
    "6. **Real-World Application**: If you were building a recommender system for a streaming music service (like Spotify), would you use explicit ratings, implicit feedback, or both? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension Challenges (Optional)\n",
    "\n",
    "If you finish early, try these challenges:\n",
    "\n",
    "1. **Hybrid System**: Combine both SVD and KNN predictions by averaging their outputs. Does this perform better than either alone?\n",
    "\n",
    "2. **Temporal Patterns**: The Jester dataset may have timestamps. Can you detect whether joke preferences change over time?\n",
    "\n",
    "3. **User Similarity**: Find the most similar user to User 1 using Pearson correlation on their joke ratings. Do they have similar taste in jokes?\n",
    "\n",
    "4. **Item-Based CF**: We used user-based collaborative filtering (find similar users). Try item-based filtering (find similar jokes) by setting `user_based=False` in KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz questions\n",
    "\n",
    "As usual, carry on with some quiz questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}